# -- Overrides the version used to determine compatibility of resources with the target Kubernetes cluster.
# This is useful when using `helm template`, because then helm will use the client version of kubectl as the Kubernetes version,
# which may or may not match your cluster's server version. Example: 'v1.24.4'. Set to null to use the version that helm
# devises.
kubeVersionOverride: null

# -- Overrides the chart's name. Used to change mimir/enterprise-metrics infix in the resource names. E.g. myRelease-mimir-ingester-1 to myRelease-nameOverride-ingester-1.
# This option is used to align resource names with Cortex, when doing a migration from Cortex to Grafana Mimir.
# Note: Grafana provided dashboards rely on the default naming and will need changes.
nameOverride: null

# -- Overrides the chart's computed fullname. Used to change the full prefix of resource names. E.g. myRelease-mimir-ingester-1 to fullnameOverride-ingester-1.
# Note: Grafana provided dashboards rely on the default naming and will need changes.
fullnameOverride: null

image:
  # -- Grafana Mimir container image repository. Note: for Grafana Enterprise Metrics use the value 'enterprise.image.repository'
  repository: grafana/mimir
  # -- Grafana Mimir container image tag. Note: for Grafana Enterprise Metrics use the value 'enterprise.image.tag'
  tag: 2.14.1
  # -- Container pull policy - shared between Grafana Mimir and Grafana Enterprise Metrics
  pullPolicy: IfNotPresent

global:
  clusterDomain: cluster.local.

  # -- Common environment variables to add to all pods directly managed by this chart.
  # scope: admin-api, alertmanager, compactor, distributor, gateway, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, smoke-test, tokengen
  extraEnv: []

  # -- Common source of environment injections to add to all pods directly managed by this chart.
  # scope: admin-api, alertmanager, compactor, distributor, gateway, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, smoke-test, tokengen
  # For example to inject values from a Secret, use:
  # extraEnvFrom:
  #   - secretRef:
  #       name: mysecret
  extraEnvFrom: []

  # -- Pod annotations for all pods directly managed by this chart. Usable for example to associate a version to 'global.extraEnv' and 'global.extraEnvFrom' and trigger a restart of the affected services.
  # scope: admin-api, alertmanager, compactor, distributor, gateway, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, tokengen
  podAnnotations: {}

  # -- Pod labels for all pods directly managed by this chart.
  # scope: admin-api, alertmanager, compactor, distributor, gateway, ingester, memcached, nginx, overrides-exporter, querier, query-frontend, query-scheduler, ruler, store-gateway, tokengen
  podLabels: {}

mimir:
  # -- Base config file for Grafana Mimir and Grafana Enterprise Metrics. Contains Helm templates that are evaulated at install/upgrade.
  # To modify the resulting configuration, either copy and alter 'mimir.config' as a whole or use the 'mimir.structuredConfig' to add and modify certain YAML elements.
  config: |
    usage_stats:
      installation_mode: helm

    activity_tracker:
      filepath: /active-query-tracker/activity.log

    alertmanager:
      data_dir: /data/alertmanager
      enable_api: true
      external_url: /alertmanager
      {{- if .Values.alertmanager.fallbackConfig }}
      fallback_config_file: /configs/alertmanager_fallback_config.yaml
      {{- end }}
      sharding_ring:
        replication_factor: 1

    # This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.
    blocks_storage:
      backend: s3
      bucket_store:
        {{- if index .Values "chunks-cache" "enabled" }}
        chunks_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.chunksCacheAddress" . }}
            max_item_size: {{ mul (index .Values "chunks-cache").maxItemMemory 1024 1024 }}
            timeout: 450ms
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "index-cache" "enabled" }}
        index_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.indexCacheAddress" . }}
            max_item_size: {{ mul (index .Values "index-cache").maxItemMemory 1024 1024 }}
            timeout: 450ms
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "metadata-cache" "enabled" }}
        metadata_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.metadataCacheAddress" . }}
            max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
            max_idle_connections: 150
        {{- end }}
        sync_dir: /data/tsdb-sync
      tsdb:
        dir: /data/tsdb
        head_compaction_interval: 15m
        wal_replay_concurrency: 3

    compactor:
      compaction_interval: 30m
      deletion_delay: 2h
      max_closing_blocks_concurrency: 2
      max_opening_blocks_concurrency: 4
      symbols_flushers_concurrency: 4
      first_level_compaction_wait_period: 25m
      data_dir: "/data/compactor"
      sharding_ring:
        wait_stability_min_duration: 1m
        heartbeat_period: 1m
        heartbeat_timeout: 4m

    distributor:
      ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m

    ingester:
      ring:
        final_sleep: 0s
        num_tokens: 512
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        heartbeat_period: 2m
        heartbeat_timeout: 10m
        replication_factor: 1

    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600

    limits:
      # Limit queries to 500 days. You can override this on a per-tenant basis.
      max_total_query_length: 12000h
      # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.
      # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.
      max_query_parallelism: 240
      # Avoid caching results newer than 10m because some samples can be delayed
      # This presents caching incomplete results
      max_cache_freshness: 10m

    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: false
      join_members:
      - dns+{{ include "mimir.fullname" . }}-gossip-ring.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.memberlistBindPort" . }}

    querier:
      # With query sharding we run more but smaller queries. We must strike a balance
      # which allows us to process more sharded queries in parallel when requested, but not overload
      # queriers during non-sharded queries.
      max_concurrent: 16

    query_scheduler:
      # Increase from default of 100 to account for queries created by query sharding
      max_outstanding_requests_per_tenant: 800

    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager
      enable_api: true
      rule_path: /data/ruler

    {{- if (index .Values "metadata-cache" "enabled") }}
    ruler_storage:
      {{- if index .Values "metadata-cache" "enabled" }}
      cache:
        backend: memcached
        memcached:
          addresses: {{ include "mimir.metadataCacheAddress" . }}
          max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
      {{- end }}
    {{- end }}

    runtime_config:
      file: /var/{{ include "mimir.name" . }}/runtime.yaml

    store_gateway:
      sharding_ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m
        replication_factor: 1
        wait_stability_min_duration: 1m
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false

  # -- Additional structured values on top of the text based 'mimir.config'. Applied after the text based config is evaluated for templates. Enables adding and modifying YAML elements in the evaulated 'mimir.config'.
  #
  # Additionally, consider the optional "insecure_skip_verify" key below, it allows you to skip_verify_false in case the s3_endpoint certificate is not trusted.
  # For more information see https://grafana.com/docs/mimir/latest/references/configuration-parameters/
  #
  # Example:
  #
  # structuredConfig:
  #   common:
  #     storage:
  #       backend: s3
  #       s3:
  #         bucket_name: "${BUCKET_NAME}"
  #         endpoint: "${BUCKET_HOST}:${BUCKET_PORT}"
  #         access_key_id: "${AWS_ACCESS_KEY_ID}" # This is a secret injected via an environment variable
  #         secret_access_key: "${AWS_SECRET_ACCESS_KEY}" # This is a secret injected via an environment variable
  #         http:
  #           insecure_skip_verify: true
  structuredConfig: {}

service:
  annotations: {}
  labels: {}

ingress:
    # -- Specifies whether an ingress for the nginx should be created
    enabled: false
    # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
    # ingressClassName: nginx
    # -- Annotations for the nginx ingress
    annotations: {}
    # -- Hosts configuration for the nginx ingress
    hosts:
      - host: nginx.mimir.example.com
        paths:
          - path: /
            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
            # pathType: Prefix
    paths:
      - path: /
        pathType: Prefix
    # -- TLS configuration for the nginx ingress
    tls: {}

alertmanager:
  enabled: true
  # -- Total number of replicas for the alertmanager across all availability zones
  # If alertmanager.zoneAwareReplication.enabled=false, this number is taken as is.
  # Otherwise each zone starts `ceil(replicas / number_of_zones)` number of pods.
  #   E.g. if 'replicas' is set to 4 and there are 3 zones, then 4/3=1.33 and after rounding up it means 2 pods per zone are started.
  replicas: 1

  statefulSet:
    enabled: true

  service:
    annotations: {}
    labels: {}

  # -- Optionally set the scheduler for pods of the alertmanager
  schedulerName: ""

  resources:
    requests:
      cpu: 10m
      memory: 32Mi

  # -- Fallback config for alertmanager.
  # When a tenant doesn't have an Alertmanager configuration, the Grafana Mimir Alertmanager uses the fallback configuration.
  fallbackConfig: |
    receivers:
        - name: default-receiver
    route:
        receiver: default-receiver

chunks-cache:
  # -- Specifies whether memcached based chunks-cache should be enabled
  enabled: false

  # -- Total number of chunks-cache replicas
  replicas: 1

  # -- Port of the chunks-cache service
  port: 11211

  # -- Amount of memory allocated to chunks-cache for object storage (in MB).
  allocatedMemory: 8192

  # -- Maximum item memory for chunks-cache (in MB).
  maxItemMemory: 1

  # -- Maximum number of connections allowed
  connectionLimit: 16384

  # -- Extra init containers for chunks-cache pods
  initContainers: []

  # -- Annotations for the chunks-cache pods
  annotations: {}
  # -- Node selector for chunks-cache pods
  nodeSelector: {}
  # -- Affinity for chunks-cache pods
  affinity: {}

  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints: {}
  #  maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway

  # -- Tolerations for chunks-cache pods
  tolerations: []
  # -- Pod Disruption Budget
  podDisruptionBudget:
    maxUnavailable: 1
  # -- The name of the PriorityClass for chunks-cache pods
  priorityClassName: null
  # -- Labels for chunks-cache pods
  podLabels: {}
  # -- Annotations for chunks-cache pods
  podAnnotations: {}
  # -- Management policy for chunks-cache pods
  podManagementPolicy: Parallel
  # -- Grace period to allow the chunks-cache to shutdown before it is killed
  terminationGracePeriodSeconds: 60

  # -- Stateful chunks-cache strategy
  statefulStrategy:
    type: RollingUpdate

  # -- Add extended options for chunks-cache memcached container. The format is the same as for the memcached -o/--extend flag.
  # Example:
  # extraExtendedOptions: 'tls,no_hashexpand'
  extraExtendedOptions: ""

  # -- Additional CLI args for chunks-cache
  extraArgs: {}

  # -- Additional containers to be added to the chunks-cache pod.
  extraContainers: []

  # -- Additional volumes to be added to the chunks-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumes:
  # - name: extra-volume
  #   secret:
  #    secretName: extra-volume-secret
  extraVolumes: []

  # -- Additional volume mounts to be added to the chunks-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumeMounts:
  # - name: extra-volume
  #   mountPath: /etc/extra-volume
  #   readOnly: true
  extraVolumeMounts: []

  # -- Resource requests and limits for the chunks-cache
  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
  resources: null

  # -- Service annotations and labels
  service:
    annotations: {}
    labels: {}

index-cache:
  # -- Specifies whether memcached based index-cache should be enabled
  enabled: false

  # -- Total number of index-cache replicas
  replicas: 1

  # -- Port of the index-cache service
  port: 11211

  # -- Amount of memory allocated to index-cache for object storage (in MB).
  allocatedMemory: 2048

  # -- Maximum item index-cache for memcached (in MB).
  maxItemMemory: 5

  # -- Maximum number of connections allowed
  connectionLimit: 16384

  # -- Extra init containers for index-cache pods
  initContainers: []

  # -- Annotations for the index-cache pods
  annotations: {}
  # -- Node selector for index-cache pods
  nodeSelector: {}
  # -- Affinity for index-cache pods
  affinity: {}

  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints: {}
  #  maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway

  # -- Tolerations for index-cache pods
  tolerations: []
  # -- Pod Disruption Budget
  podDisruptionBudget:
    maxUnavailable: 1
  # -- The name of the PriorityClass for index-cache pods
  priorityClassName: null
  # -- Labels for index-cache pods
  podLabels: {}
  # -- Annotations for index-cache pods
  podAnnotations: {}
  # -- Management policy for index-cache pods
  podManagementPolicy: Parallel
  # -- Grace period to allow the index-cache to shutdown before it is killed
  terminationGracePeriodSeconds: 60

  # -- Stateful index-cache strategy
  statefulStrategy:
    type: RollingUpdate

  # -- Add extended options for index-cache memcached container. The format is the same as for the memcached -o/--extend flag.
  # Example:
  # extraExtendedOptions: 'tls,modern,track_sizes'
  extraExtendedOptions: ""

  # -- Additional CLI args for index-cache
  extraArgs: {}

  # -- Additional containers to be added to the index-cache pod.
  extraContainers: []

  # -- Additional volumes to be added to the index-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumes:
  # - name: extra-volume
  #   secret:
  #    secretName: extra-volume-secret
  extraVolumes: []

  # -- Additional volume mounts to be added to the index-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumeMounts:
  # - name: extra-volume
  #   mountPath: /etc/extra-volume
  #   readOnly: true
  extraVolumeMounts: []

  # -- Resource requests and limits for the index-cache
  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
  resources: null

  # -- Service annotations and labels
  service:
    annotations: {}
    labels: {}

metadata-cache:
  # -- Specifies whether memcached based metadata-cache should be enabled
  enabled: false

  # -- Total number of metadata-cache replicas
  replicas: 1

  # -- Port of the metadata-cache service
  port: 11211

  # -- Amount of memory allocated to metadata-cache for object storage (in MB).
  allocatedMemory: 512

  # -- Maximum item metadata-cache for memcached (in MB).
  maxItemMemory: 1

  # -- Maximum number of connections allowed
  connectionLimit: 16384

  # -- Extra init containers for metadata-cache pods
  initContainers: []

  # -- Annotations for the metadata-cache pods
  annotations: {}
  # -- Node selector for metadata-cache pods
  nodeSelector: {}
  # -- Affinity for metadata-cache pods
  affinity: {}

  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints: {}
  #  maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway

  # -- Tolerations for metadata-cache pods
  tolerations: []
  # -- Pod Disruption Budget
  podDisruptionBudget:
    maxUnavailable: 1
  # -- The name of the PriorityClass for metadata-cache pods
  priorityClassName: null
  # -- Labels for metadata-cache pods
  podLabels: {}
  # -- Annotations for metadata-cache pods
  podAnnotations: {}
  # -- Management policy for metadata-cache pods
  podManagementPolicy: Parallel
  # -- Grace period to allow the metadata-cache to shutdown before it is killed
  terminationGracePeriodSeconds: 60

  # -- Stateful metadata-cache strategy
  statefulStrategy:
    type: RollingUpdate

  # -- Add extended options for metadata-cache memcached container. The format is the same as for the memcached -o/--extend flag.
  # Example:
  # extraExtendedOptions: 'tls,modern,track_sizes'
  extraExtendedOptions: ""

  # -- Additional CLI args for metadata-cache
  extraArgs: {}

  # -- Additional containers to be added to the metadata-cache pod.
  extraContainers: []

  # -- Additional volumes to be added to the metadata-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumes:
  # - name: extra-volume
  #   secret:
  #    secretName: extra-volume-secret
  extraVolumes: []

  # -- Additional volume mounts to be added to the metadata-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumeMounts:
  # - name: extra-volume
  #   mountPath: /etc/extra-volume
  #   readOnly: true
  extraVolumeMounts: []

  # -- Resource requests and limits for the metadata-cache
  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
  resources: null

  # -- Service annotations and labels
  service:
    annotations: {}
    labels: {}

results-cache:
  # -- Specifies whether memcached based results-cache should be enabled
  enabled: false

  # -- Total number of results-cache replicas
  replicas: 1

  # -- Port of the results-cache service
  port: 11211

  # -- Amount of memory allocated to results-cache for object storage (in MB).
  allocatedMemory: 512

  # -- Maximum item results-cache for memcached (in MB).
  maxItemMemory: 5

  # -- Maximum number of connections allowed
  connectionLimit: 16384

  # -- Extra init containers for results-cache pods
  initContainers: []

  # -- Annotations for the results-cache pods
  annotations: {}
  # -- Node selector for results-cache pods
  nodeSelector: {}
  # -- Affinity for results-cache pods
  affinity: {}

  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints: {}
  #  maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway

  # -- Tolerations for results-cache pods
  tolerations: []
  # -- Pod Disruption Budget
  podDisruptionBudget:
    maxUnavailable: 1
  # -- The name of the PriorityClass for results-cache pods
  priorityClassName: null
  # -- Labels for results-cache pods
  podLabels: {}
  # -- Annotations for results-cache pods
  podAnnotations: {}
  # -- Management policy for results-cache pods
  podManagementPolicy: Parallel
  # -- Grace period to allow the results-cache to shutdown before it is killed
  terminationGracePeriodSeconds: 60

  # -- Stateful results-cache strategy
  statefulStrategy:
    type: RollingUpdate

  # -- Add extended options for results-cache memcached container. The format is the same as for the memcached -o/--extend flag.
  # Example:
  # extraExtendedOptions: 'tls,modern,track_sizes'
  extraExtendedOptions: ""

  # -- Additional CLI args for results-cache
  extraArgs: {}

  # -- Additional containers to be added to the results-cache pod.
  extraContainers: []

  # -- Additional volumes to be added to the results-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumes:
  # - name: extra-volume
  #   secret:
  #    secretName: extra-volume-secret
  extraVolumes: []

  # -- Additional volume mounts to be added to the results-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumeMounts:
  # - name: extra-volume
  #   mountPath: /etc/extra-volume
  #   readOnly: true
  extraVolumeMounts: []

  # -- Resource requests and limits for the results-cache
  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
  resources: null

  # -- Service annotations and labels
  service:
    annotations: {}
    labels: {}

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway

nodeSelector: {}

affinity: {}